"""ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ½Ñ‹Ğµ Ñ…ĞµĞ½Ğ´Ğ»ĞµÑ€Ñ‹: /start, /help, /img, /reset, /stats, /stat, /models, /setmodel."""
from __future__ import annotations

from aiogram import Router, F
from aiogram.types import Message, CallbackQuery, InlineKeyboardMarkup, InlineKeyboardButton
from aiogram.fsm.state import State, StatesGroup
from aiogram.fsm.context import FSMContext
from aiogram.types import ReplyKeyboardRemove
import asyncio

from pathlib import Path

from bot.config import settings, VERSION
from bot.keyboards import main_kb
from bot.utils.openai import OpenAIClient
from bot.utils.db import get_conn, get_user_display_name
from bot.utils.progress import show_progress_indicator
from bot.utils.html import send_long_html_message
from bot.utils.errors import error_handler

router = Router()


class ImgGenStates(StatesGroup):
    waiting_for_prompt = State()
    waiting_for_format = State()


# â€”â€”â€” /start â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” #
@router.message(F.text == "/start")
async def cmd_start(msg: Message):
    """Ğ¡Ñ‚Ğ°Ñ€Ñ‚Ğ¾Ğ²Ğ¾Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸ĞµĞ¼."""
    welcome_text = (
        f"ğŸ‘‹ ĞŸÑ€Ğ¸Ğ²ĞµÑ‚, {msg.from_user.first_name or 'Ğ´Ñ€ÑƒĞ³'}!\n\n"
        f"Ğ¯ â€” ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ OpenAI Responses API. Ğ£Ğ¼ĞµÑ:\n\n"
        f"ğŸ’¬ ĞÑ‚Ğ²ĞµÑ‡Ğ°Ñ‚ÑŒ Ğ½Ğ° questions Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ±ĞµÑĞµĞ´Ñƒ\n"
        f"ğŸ–¼ ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑĞ¼Ğ¸\n" 
        f"ğŸ¤ Ğ Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ñ‹Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ\n"
        f"ğŸ¨ Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ĞºĞ¸ Ğ¿Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ\n\n"
        f"ĞŸÑ€Ğ¾ÑÑ‚Ğ¾ Ğ½Ğ°Ğ¿Ğ¸ÑˆĞ¸Ñ‚Ğµ Ğ¼Ğ½Ğµ Ñ‡Ñ‚Ğ¾-Ğ½Ğ¸Ğ±ÑƒĞ´ÑŒ Ğ¸Ğ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ /help Ğ´Ğ»Ñ ÑĞ¿Ñ€Ğ°Ğ²ĞºĞ¸!"
    )
    await msg.answer(welcome_text, reply_markup=main_kb(msg.from_user.id == settings.admin_id))


# â€”â€”â€” /help â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” #
@router.message(F.text == "/help")
async def cmd_help(msg: Message):
    """Ğ’Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ĞºÑ€Ğ°Ñ‚ĞºÑƒÑ ÑĞ¿Ñ€Ğ°Ğ²ĞºÑƒ."""
    
    help_lines = [
        "ğŸ¤– <b>Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹:</b>",
        "",
        "ğŸ’¬ ĞŸÑ€Ğ¾ÑÑ‚Ğ¾ Ğ¿Ğ¸ÑˆĞ¸Ñ‚Ğµ Ğ¼Ğ½Ğµ â€” Ñ Ğ¾Ñ‚Ğ²ĞµÑ‡Ñƒ",
        "ğŸ–¼ ĞÑ‚Ğ¿Ñ€Ğ°Ğ²ÑŒÑ‚Ğµ Ñ„Ğ¾Ñ‚Ğ¾ Ñ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑŒÑ â€” Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ",
        "ğŸ¤ ĞÑ‚Ğ¿Ñ€Ğ°Ğ²ÑŒÑ‚Ğµ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ¾Ğµ â€” Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ñ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‡Ñƒ",
        "ğŸ“„ ĞÑ‚Ğ¿Ñ€Ğ°Ğ²ÑŒÑ‚Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ â€” Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ",
        "",
        "<b>ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹:</b>",
        "/start â€” Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ±Ğ¾Ñ‚Ğµ",
        "/help â€” ÑÑ‚Ğ° ÑĞ¿Ñ€Ğ°Ğ²ĞºĞ°",
        "/img &lt;Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚&gt; â€” ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ĞºÑƒ",
        "/cancel â€” Ğ¾Ñ‚Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ñ‚ĞµĞºÑƒÑ‰ÑƒÑ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ",
        "/reset â€” Ğ¾Ñ‡Ğ¸ÑÑ‚Ğ¸Ñ‚ÑŒ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°",
        "/stats â€” Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ñ‹",
    ]
    
    if msg.from_user.id == settings.admin_id:
        help_lines.extend([
            "",
            "<b>ĞĞ´Ğ¼Ğ¸Ğ½ÑĞºĞ¸Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹:</b>",
            "/stat â€” Ğ¾Ğ±Ñ‰Ğ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°",
            "/models â€” Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
            "/setmodel â€” Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ñ‚ĞµĞºÑƒÑ‰ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ",
            "/checkmodel â€” Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
            "/limits â€” Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ rate limits",
            "/status â€” ÑÑ‚Ğ°Ñ‚ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸ ÑĞ»ÑƒĞ¶Ğ±",
            "/update â€” Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ñ‚Ğ°",
            "/pricing â€” Ñ†ĞµĞ½Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹"
        ])
    
    help_text = "\n".join(help_lines)
    await send_long_html_message(msg, help_text)


# â€”â€”â€” /pricing (Ğ°Ğ´Ğ¼Ğ¸Ğ½) â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” #
@router.message(F.text == "/pricing")
@error_handler("pricing_command")
async def cmd_pricing(msg: Message):
    if msg.from_user.id != settings.admin_id:
        return
    from bot.utils.openai.models import ModelsManager
    models = await ModelsManager.get_available_models()
    lines = ["ğŸ’µ <b>Ğ¦ĞµĞ½Ñ‹ Ğ·Ğ° 1k Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²</b> (input / cached_input / output):\n"]
    for m in models:
        prices = ModelsManager.get_model_prices(m['id'])
        inp = prices.get('input')
        cached = prices.get('cached_input')
        out = prices.get('output')
        if cached is not None:
            lines.append(f"â€¢ <code>{m['id']}</code>: ${inp:.5f} / ${cached:.5f} / ${out:.5f}")
        else:
            lines.append(f"â€¢ <code>{m['id']}</code>: ${inp:.5f} / ${out:.5f}")
    await send_long_html_message(msg, "\n".join(lines))


# â€”â€”â€” /status (Ğ°Ğ´Ğ¼Ğ¸Ğ½) â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” #
@router.message(F.text == "/status")
async def cmd_status(msg: Message):
    """ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ñ‚ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ğ°Ğ´Ğ¼Ğ¸Ğ½Ğ°)."""
    if msg.from_user.id != settings.admin_id:
        return

    import subprocess
    import os
    import platform

    # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°
    bot_dir = Path(__file__).parent.parent  # Ğ¸Ğ· bot/handlers/ Ğ² bot/
    project_root = bot_dir.parent  # Ğ¸Ğ· bot/ Ğ² ĞºĞ¾Ñ€ĞµĞ½ÑŒ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°

    is_windows = settings.is_windows or platform.system().lower() == 'windows'

    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ„Ğ°Ğ¹Ğ» Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ (single-instance)
    lock_file = project_root / "gpttg-bot.lock"
    lock_status = "ğŸ”“ ĞÑ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚"
    if lock_file.exists():
        try:
            with open(lock_file, 'r', encoding='utf-8') as f:
                lock_pid_str = f.read().strip()
            lock_pid = int(lock_pid_str)

            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, Ğ°ĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ‘Ğ•Ğ— Ğ¿Ğ¾ÑÑ‹Ğ»ĞºĞ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ½Ğ° Windows
            running = False
            if is_windows:
                try:
                    import ctypes
                    PROCESS_QUERY_LIMITED_INFORMATION = 0x1000
                    handle = ctypes.windll.kernel32.OpenProcess(PROCESS_QUERY_LIMITED_INFORMATION, False, lock_pid)
                    if handle:
                        ctypes.windll.kernel32.CloseHandle(handle)
                        running = True
                    else:
                        running = False
                except Exception:
                    running = False
            else:
                try:
                    os.kill(lock_pid, 0)  # POSIX: 0 Ğ½Ğµ Ğ¿Ğ¾ÑÑ‹Ğ»Ğ°ĞµÑ‚ ÑĞ¸Ğ³Ğ½Ğ°Ğ», Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚
                    running = True
                except ProcessLookupError:
                    running = False
                except PermissionError:
                    running = True

            lock_status = f"ğŸ”’ ĞĞºÑ‚Ğ¸Ğ²Ğ½Ğ° (PID: {lock_pid}, Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ: {'Ğ¶Ğ¸Ğ²' if running else 'Ğ¼ĞµÑ€Ñ‚Ğ²'})"
        except Exception:
            lock_status = "âš ï¸ Ğ¤Ğ°Ğ¹Ğ» ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚, Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ"

    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ±Ğ¾Ñ‚Ğ°
    process_count = "n/a"
    if not is_windows:
        try:
            result = subprocess.run(['pgrep', '-f', 'bot.main'], capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                pids = [p for p in result.stdout.strip().split('\n') if p]
                process_count = len(pids)
            else:
                process_count = 0
        except Exception as e:
            process_count = f"Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {str(e)[:30]}..."

    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ systemd ÑĞ»ÑƒĞ¶Ğ±Ñ‹ (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Linux)
    systemd_services = []
    if not is_windows:
        services_to_check = [
            ("gpttg-bot.service", "ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ ÑĞ»ÑƒĞ¶Ğ±Ğ° Ğ±Ğ¾Ñ‚Ğ°"),
            ("gpttg-update.service", "Ğ¡Ğ»ÑƒĞ¶Ğ±Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ"),
            ("gpttg-update.timer", "Ğ¢Ğ°Ğ¹Ğ¼ĞµÑ€ Ğ°Ğ²Ñ‚Ğ¾Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ")
        ]

        for service_name, description in services_to_check:
            try:
                result = subprocess.run(['systemctl', 'status', service_name], capture_output=True, text=True, timeout=5)
                status_output = result.stdout.strip() if result.returncode == 0 else result.stderr.strip()
                if "Active: active" in status_output:
                    icon = "âœ…"
                    status_text = "Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ°"
                elif "Active: inactive" in status_output and "Result: exit-code" not in status_output:
                    icon = "âš«"
                    status_text = "Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ (oneshot)"
                elif "failed" in status_output or "Result: exit-code" in status_output:
                    icon = "âŒ"
                    status_text = "ÑĞ±Ğ¾Ğ¹"
                else:
                    icon = "âš ï¸"
                    status_text = "Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾"
                systemd_services.append(f"{icon} {service_name}: {status_text}")
            except Exception as e:
                systemd_services.append(f"â“ {service_name}: Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ({str(e)[:30]})")

    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹
    version_files = []
    root_files = [".env", "update_bot.sh"]
    for file_name in root_files:
        file_path = project_root / file_name
        if file_path.exists():
            try:
                size = file_path.stat().st_size
                version_files.append(f"âœ… {file_name} ({size} Ğ±Ğ°Ğ¹Ñ‚)")
            except Exception:
                version_files.append(f"âš ï¸ {file_name} (Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ)")
        else:
            version_files.append(f"âŒ {file_name} (Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚)")

    # Ğ‘Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    db_file = bot_dir / "bot.sqlite"
    if db_file.exists():
        try:
            size = db_file.stat().st_size
            version_files.append(f"âœ… bot.sqlite ({size} Ğ±Ğ°Ğ¹Ñ‚)")
        except Exception:
            version_files.append(f"âš ï¸ bot.sqlite (Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ)")
    else:
        version_files.append(f"âŒ bot.sqlite (Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚)")

    # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ ÑÑ‚Ğ°Ñ‚ÑƒÑ
    status_text = (
        f"ğŸ”§ <b>Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹:</b>\n\n"
        f"ğŸ“‹ Ğ’ĞµÑ€ÑĞ¸Ñ Ğ±Ğ¾Ñ‚Ğ°: <code>{VERSION}</code>\n"
        f"ğŸ–¥ï¸ ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°: <code>{settings.platform} ({'dev' if settings.is_development else 'prod'})</code>\n"
        f"ğŸ”’ Ğ‘Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ° ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ°: {lock_status}\n"
    )
    if not is_windows:
        status_text += f"âš™ï¸ ĞŸÑ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² bot.main: <code>{str(process_count)}</code>\n\n"

    if systemd_services:
        status_text += f"ğŸ”§ <b>Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ ÑĞ»ÑƒĞ¶Ğ±Ñ‹:</b>\n"
        for service_info in systemd_services:
            status_text += f"  {service_info}\n"
        status_text += "\n"

    status_text += f"ğŸ’¾ <b>Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹:</b>\n"
    for file_info in version_files:
        status_text += f"  {file_info}\n"

    await send_long_html_message(msg, status_text)


# â€”â€”â€” /models (Ğ°Ğ´Ğ¼Ğ¸Ğ½) â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” #
@router.message(F.text == "/models")
@error_handler("models_command")
async def cmd_models(msg: Message):
    """ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ğ°Ğ´Ğ¼Ğ¸Ğ½Ğ°)."""
    if msg.from_user.id != settings.admin_id:
        return

    current_model = await OpenAIClient.get_current_model()
    models = await OpenAIClient.get_available_models()
    
    models_text = f"ğŸ¤– <b>Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸:</b>\n\n"
    models_text += f"ğŸ”¸ <b>Ğ¢ĞµĞºÑƒÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ:</b> <code>{current_model}</code>\n\n"
    
    for model in models:
        status = "âœ…" if model['id'] == current_model else "âšª"
        models_text += f"{status} <code>{model['id']}</code>\n"
    
    models_text += f"\nğŸ’¡ <i>ĞĞµ Ğ²ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‚ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ! ĞŸÑ€Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ğ½ĞµÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ñ‚ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑÑ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ gpt-4Ğ¾-mini</i>"
    models_text += f"\nĞ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ /setmodel Ğ´Ğ»Ñ ÑĞ¼ĞµĞ½Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸"
    
    await send_long_html_message(msg, models_text)
        


# â€”â€”â€” /setmodel (Ğ°Ğ´Ğ¼Ğ¸Ğ½) â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” #
@router.message(F.text == "/setmodel")
@error_handler("setmodel_command")
async def cmd_setmodel(msg: Message):
    """ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ inline ĞºĞ»Ğ°Ğ²Ğ¸Ğ°Ñ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ğ°Ğ´Ğ¼Ğ¸Ğ½Ğ°)."""
    if msg.from_user.id != settings.admin_id:
        return

    current_model = await OpenAIClient.get_current_model()
    models = await OpenAIClient.get_available_models()
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ inline ĞºĞ»Ğ°Ğ²Ğ¸Ğ°Ñ‚ÑƒÑ€Ñƒ
    keyboard = InlineKeyboardMarkup(inline_keyboard=[])
    
    for model in models:
        status = "âœ… " if model['id'] == current_model else ""
        keyboard.inline_keyboard.append([
            InlineKeyboardButton(
                text=f"{status}{model['id']}",
                callback_data=f"setmodel:{model['id']}"
            )
        ])
    
    await msg.answer(
        f"ğŸ¤– <b>Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ:</b>\n\nĞ¢ĞµĞºÑƒÑ‰Ğ°Ñ: <code>{current_model}</code>",
        reply_markup=keyboard
    )


# â€”â€”â€” Callback Ğ´Ğ»Ñ ÑĞ¼ĞµĞ½Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” #
@router.callback_query(lambda c: c.data and c.data.startswith("setmodel:"))
@error_handler("setmodel_callback")
async def callback_setmodel(callback: CallbackQuery):
    """ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· inline ĞºĞ½Ğ¾Ğ¿ĞºÑƒ."""
    if callback.from_user.id != settings.admin_id:
        await callback.answer("âŒ ĞĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¿Ñ€Ğ°Ğ²", show_alert=True)
        return

    model_id = callback.data.split(":", 1)[1]
    
    # Ğ£ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
    await OpenAIClient.set_current_model(model_id)
    
    await callback.message.edit_text(
        f"âœ… <b>ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ°!</b>\n\nĞ¢ĞµĞºÑƒÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ: <code>{model_id}</code>\n\n"
        f"Ğ’ÑĞµ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ±ÑƒĞ´ÑƒÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ."
    )
    
    await callback.answer("âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ°!")


# â€”â€”â€” /reset â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” #
@router.message(F.text.startswith("/reset"))
async def cmd_reset(msg: Message, state: FSMContext):
    """Ğ£Ğ´Ğ°Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½Ğ½Ñ‹Ğ¹ previous_response_id Ğ¸ Ğ²ÑĞµ Ñ„Ğ°Ğ¹Ğ»Ñ‹ OpenAI Ğ´Ğ»Ñ Ñ‡Ğ°Ñ‚Ğ°. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ñ‡Ğ¸Ñ‰Ğ°ĞµÑ‚ Ğ²ÑĞµ Ğ½Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ñ‡Ğ°Ñ‚Ğ°."""
    # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ¸Ğ· OpenAI Ğ¸ Ğ‘Ğ”
    await OpenAIClient.delete_files_by_chat(msg.chat.id)
    # ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ñ‡Ğ°Ñ‚Ğ° Ğ¸ Ğ½Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ
    async with get_conn() as db:
        await db.execute(
            "DELETE FROM chat_history WHERE chat_id = ?",
            (msg.chat.id,)
        )
        await db.execute(
            "DELETE FROM reminders WHERE chat_id = ?",
            (msg.chat.id,)
        )
        await db.commit()
    await msg.answer("ğŸ—‘ Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ, Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ¸ Ğ²ÑĞµ Ğ½Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ñ‹! Ğ¡Ğ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ½Ğ°Ñ‡Ğ½ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³.", 
                    reply_markup=main_kb(msg.from_user.id == settings.admin_id))


# â€”â€”â€” /stats â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” #
@router.message(F.text.startswith("/stats"))
async def cmd_stats(msg: Message):
    """ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ñ‹ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ."""
    
    async with get_conn() as db:
        # ĞĞ±Ñ‰Ğ¸Ğµ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ñ‹
        cur = await db.execute(
            "SELECT COALESCE(SUM(cost),0) FROM usage WHERE user_id = ?",
            (msg.from_user.id,),
        )
        (total,) = await cur.fetchone()
        
        # Ğ Ğ°Ğ·Ğ±Ğ¸Ğ²ĞºĞ° Ğ¿Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼
        cur = await db.execute(
            """SELECT model, COUNT(*) as requests, COALESCE(SUM(cost),0) as model_cost 
               FROM usage WHERE user_id = ? GROUP BY model ORDER BY model_cost DESC""",
            (msg.from_user.id,)
        )
        models = await cur.fetchall()
    
    # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ
    stats_text = f"ğŸ“Š <b>Ğ’Ğ°ÑˆĞ° ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°:</b>\n\nĞĞ±Ñ‰Ğ¸Ğµ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ñ‹: <code>${total:.4f}</code>\n\n"
    
    if models:
        stats_text += "<b>ĞŸĞ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼:</b>\n"
        for model, requests, cost in models:
            stats_text += f"â€¢ <code>{model}</code>: {requests} Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² â€” <code>${cost:.4f}</code>\n"
    else:
        stats_text += "ĞŸĞ¾ĞºĞ° Ğ½ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ± Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸."
    
    await send_long_html_message(msg, stats_text)


# â€”â€”â€” /stat (Ğ°Ğ´Ğ¼Ğ¸Ğ½) â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” #
@router.message(F.text.startswith("/stat"))
async def cmd_stat(msg: Message):
    """ĞĞ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ğ°Ğ´Ğ¼Ğ¸Ğ½Ğ¸ÑÑ‚Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ°."""
    if msg.from_user.id != settings.admin_id:
        return

    async with get_conn() as db:
        current_model = await OpenAIClient.get_current_model()
        
        (day_total,) = await (await db.execute(
            "SELECT COALESCE(SUM(cost),0) FROM usage WHERE ts >= DATETIME('now','-1 day')",
        )).fetchone()

        (week_total,) = await (await db.execute(
            "SELECT COALESCE(SUM(cost),0) FROM usage WHERE ts >= DATETIME('now','-7 day')",
        )).fetchone()

        (month_total,) = await (await db.execute(
            "SELECT COALESCE(SUM(cost),0) FROM usage WHERE ts >= DATETIME('now','-1 month')",
        )).fetchone()

        rows = await (await db.execute(
            "SELECT user_id, SUM(cost) AS c FROM usage GROUP BY user_id ORDER BY c DESC LIMIT 10",
        )).fetchall()

    # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ñ‚Ğ¾Ğ¿ Ñ Ğ¸Ğ¼ĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹
    leaderboard_lines = []
    for user_id, cost in rows:
        display_name = await get_user_display_name(user_id)
        leaderboard_lines.append(f"â€¢ {display_name} â€” <code>${cost:.4f}</code>")
    
    leaderboard = "\n".join(leaderboard_lines) or "â€”"

    # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ
    stat_text = (
        f"ğŸ“ˆ <b>ĞĞ±Ñ‰Ğ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°:</b>\n\n"
        f"ğŸ¤– Ğ¢ĞµĞºÑƒÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ: <code>{current_model}</code>\n\n"
        f"ğŸ“… Ğ—Ğ° ÑÑƒÑ‚ĞºĞ¸: <code>${day_total:.4f}</code>\n"
        f"ğŸ“… Ğ—Ğ° Ğ½ĞµĞ´ĞµĞ»Ñ: <code>${week_total:.4f}</code>\n"
        f"ğŸ“… Ğ—Ğ° Ğ¼ĞµÑÑÑ†: <code>${month_total:.4f}</code>\n\n"
        f"ğŸ† <b>Ğ¢Ğ¾Ğ¿-10 Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹:</b>\n{leaderboard}"
    )
    
    await send_long_html_message(msg, stat_text)


# â€”â€”â€” /img â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” #
@router.message(F.text == "/img")
async def cmd_img(msg: Message, state: FSMContext):
    """Ğ—Ğ°Ğ¿Ñ€Ğ°ÑˆĞ¸Ğ²Ğ°ĞµĞ¼ Ñƒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ĞºĞ¸."""
    await msg.answer(
        "ğŸ¨ <b>Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ</b>\n\n"
        "ĞĞ¿Ğ¸ÑˆĞ¸Ñ‚Ğµ, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ñ€Ğ¸ÑĞ¾Ğ²Ğ°Ñ‚ÑŒ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€: 'ĞšĞ¾Ñ‚ Ğ² ĞºĞ¾ÑĞ¼Ğ¾ÑĞµ', 'Ğ“Ğ¾Ñ€Ñ‹ Ğ½Ğ° Ğ·Ğ°ĞºĞ°Ñ‚Ğµ', ...).\n\n"
        "ğŸ’¡ Ğ”Ğ»Ñ Ğ¾Ñ‚Ğ¼ĞµĞ½Ñ‹ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ÑŒÑ‚Ğµ: <code>/cancel</code> Ğ¸Ğ»Ğ¸ <code>Ğ¾Ñ‚Ğ¼ĞµĞ½Ğ°</code>",
        reply_markup=ReplyKeyboardRemove()
    )
    await state.set_state(ImgGenStates.waiting_for_prompt)


@router.message(ImgGenStates.waiting_for_prompt)
async def imggen_get_prompt(msg: Message, state: FSMContext):
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚, ÑĞ¿Ñ€Ğ°ÑˆĞ¸Ğ²Ğ°ĞµĞ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚."""
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ¾Ñ‚Ğ¼ĞµĞ½Ñ‹
    if msg.text and msg.text.lower() in ['/cancel', '/Ğ¾Ñ‚Ğ¼ĞµĞ½Ğ°', 'Ğ¾Ñ‚Ğ¼ĞµĞ½Ğ°', 'cancel']:
        await msg.answer("âŒ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ¼ĞµĞ½ĞµĞ½Ğ°.", 
                        reply_markup=main_kb(msg.from_user.id == settings.admin_id))
        await state.clear()
        return
    
    await state.update_data(prompt=msg.text)
    kb = InlineKeyboardMarkup(
        inline_keyboard=[
            [InlineKeyboardButton(text="Ğ’ĞµÑ€Ñ‚Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ (1024x1792)", callback_data="img_fmt_vert")],
            [InlineKeyboardButton(text="Ğ“Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ (1792x1024)", callback_data="img_fmt_horiz")],
            [InlineKeyboardButton(text="ĞšĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğ¹ (1024x1024)", callback_data="img_fmt_square")],
            [InlineKeyboardButton(text="âŒ ĞÑ‚Ğ¼ĞµĞ½Ğ°", callback_data="img_cancel")]
        ]
    )
    await msg.answer("Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ:", reply_markup=kb)
    await state.set_state(ImgGenStates.waiting_for_format)


@router.callback_query(ImgGenStates.waiting_for_format)
@error_handler("img_generation")
async def imggen_get_format(callback: CallbackQuery, state: FSMContext):
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ĞºÑƒ."""
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, ĞµÑĞ»Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ½Ğ°Ğ¶Ğ°Ğ» Ğ¾Ñ‚Ğ¼ĞµĞ½Ñƒ
    if callback.data == "img_cancel":
        await callback.answer("âŒ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ¼ĞµĞ½ĞµĞ½Ğ°")
        await callback.message.edit_text("âŒ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ¼ĞµĞ½ĞµĞ½Ğ°.")
        await state.clear()
        return
    
    data = await state.get_data()
    prompt = data.get("prompt") or "Ğ¡Ğ¼ĞµÑˆĞ½Ğ¾Ğ¹ ĞºĞ¾Ñ‚"
    if callback.data == "img_fmt_vert":
        size = "1024x1792"
    elif callback.data == "img_fmt_horiz":
        size = "1792x1024"
    elif callback.data == "img_fmt_square":
        size = "1024x1024"
    else:
        size = "1024x1024"
    # Ğ¡Ñ€Ğ°Ğ·Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµĞ¼ Ğ½Ğ° callback, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Telegram Ğ½Ğµ Ğ²Ñ‹Ğ´Ğ°Ğ» Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ
    await callback.answer()
    progress_task = None
    try:
        progress_task = asyncio.create_task(
            show_progress_indicator(callback.bot, callback.message.chat.id, max_time=60)
        )
        url = await OpenAIClient.dalle(prompt, size, callback.message.chat.id, callback.from_user.id)
        if not url:
            raise ValueError("ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ.")
        await callback.message.answer_photo(url, caption=f"ğŸ–¼ {prompt}")
    finally:
        if progress_task and not progress_task.done():
            progress_task.cancel()
    await state.clear()


# â€”â€”â€” /cancel (Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ‚Ğ¼ĞµĞ½Ğ°) â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” #
@router.message(F.text.in_(["/cancel", "/Ğ¾Ñ‚Ğ¼ĞµĞ½Ğ°"]))
async def cmd_cancel(msg: Message, state: FSMContext):
    """Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° Ğ¾Ñ‚Ğ¼ĞµĞ½Ñ‹ Ğ´Ğ»Ñ Ğ»ÑĞ±Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ FSM."""
    current_state = await state.get_state()
    
    if current_state is None:
        await msg.answer("âŒ ĞĞµÑ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ¼ĞµĞ½Ñ‹.", 
                        reply_markup=main_kb(msg.from_user.id == settings.admin_id))
        return
    
    await state.clear()
    
    # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼, ĞºĞ°ĞºĞ°Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ±Ñ‹Ğ»Ğ° Ğ¾Ñ‚Ğ¼ĞµĞ½ĞµĞ½Ğ°
    if current_state in ["ImgGenStates:waiting_for_prompt", "ImgGenStates:waiting_for_format"]:
        operation = "Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ"
    else:
        operation = "Ñ‚ĞµĞºÑƒÑ‰Ğ°Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ"
    
    await msg.answer(f"âŒ {operation.capitalize()} Ğ¾Ñ‚Ğ¼ĞµĞ½ĞµĞ½Ğ°.", 
                    reply_markup=main_kb(msg.from_user.id == settings.admin_id))


# â€”â€”â€” /checkmodel (Ğ°Ğ´Ğ¼Ğ¸Ğ½) â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” #
@router.message(F.text == "/checkmodel")
@error_handler("checkmodel_command")
async def cmd_checkmodel(msg: Message):
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ñ‚ĞµĞºÑƒÑ‰ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ğ°Ğ´Ğ¼Ğ¸Ğ½Ğ°)."""
    if msg.from_user.id != settings.admin_id:
        return

    from bot.utils.openai.models import ModelsManager

    current_model = await ModelsManager.get_current_model()
    models = await ModelsManager.get_available_models()
    available_ids = {m['id'] for m in models}

    is_available = current_model in available_ids

    status_icon = "âœ…" if is_available else "âŒ"
    status_text = "Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°" if is_available else "Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ° ÑÑ€ĞµĞ´Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ…"

    # Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ rate limits Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
    rate_limits_info = {
        "gpt-5": "30,000 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²/Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ",
        "gpt-4o": "150,000 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²/Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", 
        "gpt-4o-mini": "200,000 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²/Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ"
    }
    
    limit_info = rate_limits_info.get(current_model, "Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾")

    response_text = (
        f"ğŸ” <b>Ğ¢ĞµĞºÑƒÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ:</b> <code>{current_model}</code>\n"
        f"{status_icon} Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ: <b>{status_text}</b>\n"
        f"ğŸ“Š Ğ›Ğ¸Ğ¼Ğ¸Ñ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²: <code>{limit_info}</code>\n\n"
        f"<b>Ğ” Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ Ñ:</b>\n"
        f"â€¢ /models â€” Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\n"
        f"â€¢ /setmodel â€” ÑĞ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ\n\n"
        f"ğŸ’¡ <b>Ğ¡Ğ¾Ğ²ĞµÑ‚:</b> gpt-4Ğ¾-mini Ğ¸Ğ¼ĞµĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¹ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²"
    )

    if not is_available:
        response_text += f"\n\nâš¡ <b>Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ñ:</b> Ğ²Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‡ĞµÑ€ĞµĞ· /setmodel"

    await send_long_html_message(msg, response_text)


# â€”â€”â€” /limits (Ğ°Ğ´Ğ¼Ğ¸Ğ½) â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” #
@router.message(F.text == "/limits")
@error_handler("limits_command")
async def cmd_limits(msg: Message):
    """ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ rate limits (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ğ°Ğ´Ğ¼Ğ¸Ğ½Ğ°)."""
    if msg.from_user.id != settings.admin_id:
        return

    from bot.utils.openai.models import ModelsManager

    current_model = await ModelsManager.get_current_model()
    
    # Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ… Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
    model_limits = {
        "gpt-5": {"tokens": "30,000/Ğ¼Ğ¸Ğ½", "requests": "500/Ğ¼Ğ¸Ğ½", "tier": "Tier 5"},
        "gpt-4o": {"tokens": "150,000/Ğ¼Ğ¸Ğ½", "requests": "10,000/Ğ¼Ğ¸Ğ½", "tier": "Tier 5"},
        "gpt-4o-mini": {"tokens": "200,000/Ğ¼Ğ¸Ğ½", "requests": "30,000/Ğ¼Ğ¸Ğ½", "tier": "Tier 5"},
        "gpt-4": {"tokens": "40,000/Ğ¼Ğ¸Ğ½", "requests": "5,000/Ğ¼Ğ¸Ğ½", "tier": "Tier 4"},
        "gpt-3.5-turbo": {"tokens": "90,000/Ğ¼Ğ¸Ğ½", "requests": "10,000/Ğ¼Ğ¸Ğ½", "tier": "Tier 4"}
    }
    
    current_limits = model_limits.get(current_model, {
        "tokens": "Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾", 
        "requests": "Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾", 
        "tier": "Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾"
    })
    
    # Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
    status_emoji = "ğŸŸ¢" if current_model in ["gpt-4o-mini", "gpt-4o"] else "ğŸŸ¡" if current_model == "gpt-5" else "ğŸ”´"
    
    response_text = (
        f"ğŸ“Š <b>Rate Limits Information</b>\n\n"
        f"ğŸ¤– <b>Ğ¢ĞµĞºÑƒÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ:</b> <code>{current_model}</code> {status_emoji}\n"
        f"ğŸ¯ <b>Ğ›Ğ¸Ğ¼Ğ¸Ñ‚Ñ‹:</b>\n"
        f"  â€¢ Ğ¢Ğ¾ĞºĞµĞ½Ñ‹: <code>{current_limits['tokens']}</code>\n"
        f"  â€¢ Ğ—Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹: <code>{current_limits['requests']}</code>\n"
        f"  â€¢ Ğ¢Ğ°Ñ€Ğ¸Ñ„: <code>{current_limits['tier']}</code>\n\n"
        f"ğŸ“ˆ <b>Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµĞ¼Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸:</b>\n"
        f"ğŸŸ¢ gpt-4Ğ¾-mini â€” 200k Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²/Ğ¼Ğ¸Ğ½ (Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€)\n"
        f"ğŸŸ¢ gpt-4Ğ¾ â€” 150k Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²/Ğ¼Ğ¸Ğ½ (Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾)\n"
        f"ğŸŸ¡ gpt-5 â€” 30k Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²/Ğ¼Ğ¸Ğ½ (Ğ½Ğ¾Ğ²ĞµĞ¹ÑˆĞ°Ñ, Ğ½Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ)\n\n"
        f"ğŸ’¡ <b>ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ñ‚Ğ°:</b>\n"
        f"  â€¢ Retry: <code>Ğ¾Ñ‚ĞºĞ»ÑÑ‡ĞµĞ½</code> âœ…\n"
        f"  â€¢ Ğ¡ĞµĞ¼Ğ°Ñ„Ğ¾Ñ€: <code>1 Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ/Ñ‡Ğ°Ñ‚, {getattr(settings, 'openai_global_concurrency', 4)} Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾</code> âœ…\n"
        f"  â€¢ Web search: <code>Ğ²ĞºĞ»ÑÑ‡ĞµĞ½</code> ğŸ”\n\n"
        f"ğŸ”§ /setmodel â€” ÑĞ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ\n"
        f"ğŸ“Š /checkmodel â€” Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ"
    )
    
    await send_long_html_message(msg, response_text)