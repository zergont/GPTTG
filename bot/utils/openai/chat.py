"""–ß–∞—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º OpenAI Responses API."""
from typing import Any, Dict, List
import openai

from bot.config import settings
from bot.utils.db import get_conn
from bot.utils.log import logger
from .base import client, oai_limiter
from .models import ModelsManager


class ChatManager:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —á–∞—Ç–æ–º —á–µ—Ä–µ–∑ OpenAI Responses API."""
    
    @staticmethod
    async def responses_request(
        chat_id: int,
        user_id: int,
        user_content: List[Dict[str, Any]],
        previous_response_id: str | None = None,
        tools: list | None = None,
        enable_web_search: bool | None = None,
        tool_choice: str | None = None
    ) -> str:
        """
        –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –≤ OpenAI Responses API.

        Args:
            chat_id: ID —á–∞—Ç–∞.
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏).
            user_content: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
            previous_response_id: ID –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –æ—Ç–≤–µ—Ç–∞ (–¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞).
            tools: –°–ø–∏—Å–æ–∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤.
            enable_web_search: –í–∫–ª—é—á–∏—Ç—å web_search tool.
            tool_choice: –í—ã–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ (auto/none/required).

        Returns:
            str: –û—Ç–≤–µ—Ç –æ—Ç OpenAI.
        """
        async with oai_limiter(chat_id):
            logger.info("–ó–∞–ø—Ä–æ—Å –≤ OpenAI (chat=%s, prev=%s)", chat_id, previous_response_id)
            
            current_model = await ModelsManager.get_current_model()
            
            if previous_response_id is None:
                async with get_conn() as db:
                    cur = await db.execute(
                        "SELECT last_response FROM chat_history WHERE chat_id = ?",
                        (chat_id,)
                    )
                    row = await cur.fetchone()
                    previous_response_id = row[0] if row else None

            input_content = []
            if previous_response_id is None:
                input_content.append({
                    "type": "message",
                    "content": settings.system_prompt,
                    "role": "system"
                })
                logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –≤ —á–∞—Ç–µ {chat_id}")
            else:
                logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è previous_response_id={previous_response_id}, —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –ø—Ä–æ–ø—É—â–µ–Ω")

            input_content.extend(user_content)

            request_params = {
                "model": current_model,
                "input": input_content,
                "previous_response_id": previous_response_id,
                "store": True,
            }

            # –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
            use_web = True if enable_web_search is None else bool(enable_web_search)
            if use_web:
                request_params.setdefault("tools", []).append({"type": "web_search"})
            if tools:
                request_params.setdefault("tools", []).extend(tools)
            if tool_choice:
                request_params["tool_choice"] = tool_choice

            # DEBUG-–∑–∞–ø—Ä–æ—Å
            if getattr(settings, "debug_mode", False):
                logger.debug(f"[DEBUG] OpenAI REQUEST: {request_params}")

            try:
                response = await client.responses.create(**request_params)
            except openai.APITimeoutError:
                logger.error("OpenAI: –ø—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞")
                return "‚è≥ –ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑."
            except openai.RateLimitError as e:
                logger.error("OpenAI: –ø—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤")
                error_details = str(e)
                remaining_tokens = "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
                reset_time = "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
                if hasattr(e, 'response') and e.response:
                    headers = getattr(e.response, 'headers', {})
                    remaining_tokens = headers.get('x-ratelimit-remaining-tokens', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                    reset_time = headers.get('x-ratelimit-reset-tokens', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                return (
                    f"‚è≥ <b>–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤ OpenAI</b>\n\n"
                    f"üî¢ –û—Å—Ç–∞–ª–æ—Å—å —Ç–æ–∫–µ–Ω–æ–≤: <code>{remaining_tokens}</code>\n"
                    f"üïí –°–±—Ä–æ—Å —á–µ—Ä–µ–∑: <code>{reset_time}</code> —Å–µ–∫\n\n"
                    f"üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:\n"
                    f"‚Ä¢ /setmodel ‚Üí gpt-4o-mini (–¥–µ—à–µ–≤–ª–µ)\n"
                    f"‚Ä¢ –ü–æ–¥–æ–∂–¥–∏—Ç–µ {reset_time} —Å–µ–∫—É–Ω–¥\n"
                    f"‚Ä¢ –£–ø—Ä–æ—Å—Ç–∏—Ç–µ –∑–∞–ø—Ä–æ—Å"
                )
            except (openai.PermissionDeniedError, openai.BadRequestError) as e:
                error_message = str(e)
                logger.warning(f"–ü—Ä–æ–±–ª–µ–º–∞ —Å –º–æ–¥–µ–ª—å—é {current_model}: {error_message}")
                from .models import ModelsManager as _MM
                await _MM.set_current_model("gpt-4o-mini")
                if "does not have access" in error_message:
                    return f"‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ –º–æ–¥–µ–ª–∏ {current_model}. –ú–æ–¥–µ–ª—å –∏–∑–º–µ–Ω–µ–Ω–∞ –Ω–∞ gpt-4o-mini. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ –∑–∞–ø—Ä–æ—Å."
                elif "not supported with the Responses API" in error_message:
                    return f"‚ùå –ú–æ–¥–µ–ª—å {current_model} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Responses API. –ú–æ–¥–µ–ª—å –∏–∑–º–µ–Ω–µ–Ω–∞ –Ω–∞ gpt-4o-mini. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ –∑–∞–ø—Ä–æ—Å."
                elif "does not support image inputs" in error_message:
                    return f"‚ùå –ú–æ–¥–µ–ª—å {current_model} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –ú–æ–¥–µ–ª—å –∏–∑–º–µ–Ω–µ–Ω–∞ –Ω–∞ gpt-4o-mini. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ –∑–∞–ø—Ä–æ—Å."
                else:
                    return f"‚ùå –ü—Ä–æ–±–ª–µ–º–∞ —Å –º–æ–¥–µ–ª—å—é {current_model}. –ú–æ–¥–µ–ª—å –∏–∑–º–µ–Ω–µ–Ω–∞ –Ω–∞ gpt-4o-mini. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ –∑–∞–ø—Ä–æ—Å."
            except Exception as e:
                logger.error(f"OpenAI:unexpected error: {e}")
                return f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)[:100]}..."

            if getattr(settings, "debug_mode", False):
                logger.debug(f"[DEBUG] OpenAI RESPONSE: {response}")

            usage = getattr(response, "usage", None)
            total_tokens = getattr(usage, "total_tokens", 0) if usage else 0

            # –°—Ç–æ–∏–º–æ—Å—Ç—å: —É—á–∏—Ç—ã–≤–∞–µ–º —Ü–µ–Ω—ã –º–æ–¥–µ–ª–∏ –∏ cached_input, –µ—Å–ª–∏ –µ—Å—Ç—å
            prices = ModelsManager.get_model_prices(getattr(response, "model", None) or current_model)
            in_price = prices.get("input", settings.openai_price_per_1k_tokens)
            out_price = prices.get("output", settings.openai_price_per_1k_tokens)
            cached_price = prices.get("cached_input", in_price)

            # –ü–æ–ø—ã—Ç–∞—Ç—å—Å—è –≤–∑—è—Ç—å —Ä–∞–∑–¥–µ–ª—å–Ω—ã–µ —Å—á—ë—Ç—á–∏–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤
            input_tokens = (
                getattr(usage, "prompt_tokens", None)
                or getattr(usage, "input_tokens", None)
            )
            output_tokens = (
                getattr(usage, "completion_tokens", None)
                or getattr(usage, "output_tokens", None)
            )
            cached_input_tokens = (
                getattr(usage, "cached_prompt_tokens", None)
                or getattr(usage, "cached_input_tokens", None)
            )

            if input_tokens is not None and output_tokens is not None:
                # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –∑–∞–∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏ –æ–±—ã—á–Ω—ã–µ –≤—Ö–æ–¥–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
                cached_t = max(0, int(cached_input_tokens)) if cached_input_tokens is not None else 0
                regular_t = max(0, int(input_tokens) - cached_t)
                cost = (regular_t / 1000.0) * in_price + (cached_t / 1000.0) * cached_price + (int(output_tokens) / 1000.0) * out_price
            else:
                # Fallback: —Å—á–∏—Ç–∞–µ–º –ø–æ total –∏ –≥–ª–æ–±–∞–ª—å–Ω–æ–π —Ü–µ–Ω–µ
                cost = total_tokens / 1000.0 * settings.openai_price_per_1k_tokens

            async with get_conn() as db:
                await db.execute(
                    "REPLACE INTO chat_history(chat_id, last_response) VALUES (?, ?)",
                    (chat_id, response.id),
                )
                await db.execute(
                    "INSERT INTO usage(chat_id, user_id, tokens, cost, model) VALUES (?, ?, ?, ?, ?)",
                    (chat_id, user_id, total_tokens, cost, getattr(response, "model", current_model)),
                )
                await db.commit()

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
            if getattr(response, "output", None):
                try:
                    for message in response.output:
                        if hasattr(message, 'content') and message.content:
                            for content_item in message.content:
                                if hasattr(content_item, 'text') and content_item.text:
                                    return content_item.text
                                elif getattr(content_item, 'type', '') == 'output_text' and hasattr(content_item, 'text'):
                                    return content_item.text
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ—Ç–≤–µ—Ç–∞: {e}")

            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞ OpenAI")
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ—Ç–≤–µ—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑."